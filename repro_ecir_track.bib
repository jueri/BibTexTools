
@incollection{azzopardi_reproducing_2019,
	address = {Cham}
	title = {Reproducing and {Generalizing} {Semantic} {Term} {Matching} in {Axiomatic} {Information} {Retrieval}},
	volume = {11437},
	isbn = {978-3-030-15711-1 978-3-030-15712-8},
	url = {http://link.springer.com/10.1007/978-3-030-15712-8_24},
	abstract = {In the framework of axiomatic information retrieval, the semantic term matching technique proposed by Fang and Zhai in SIGIR 2006 has been shown to be eﬀective in addressing the vocabulary mismatch problem, with experimental evidence provided from newswire collections. This paper reproduces and generalizes these results in Anserini, an open-source IR toolkit built on Lucene. In addition to making an implementation of axiomatic semantic term matching available on a widely-used open-source platform, we describe a series of experiments that help researchers and practitioners better understand its behavior across a number of test collections spanning newswire, web, and microblogs. Results show that axiomatic semantic term matching can be applied on top of diﬀerent base retrieval models, and that its eﬀectiveness varies across diﬀerent document genres, each requiring diﬀerent parameter settings for optimal eﬀectiveness.},
	language = {en},
	urldate = {2020-02-27},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Yang, Peilin and Lin, Jimmy},
	editor = {Azzopardi, Leif and Stein, Benno and Fuhr, Norbert and Mayr, Philipp and Hauff, Claudia and Hiemstra, Djoerd},
	year = {2019},
	doi = {10.1007/978-3-030-15712-8_24},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {ecir repro track},
	pages = {369--381},
	file = {Yang and Lin - 2019 - Reproducing and Generalizing Semantic Term Matchin.pdf:files/1143/Yang and Lin - 2019 - Reproducing and Generalizing Semantic Term Matchin.pdf:application/pdf},
}

@incollection{azzopardi_simple_2019,
	address = {Cham},
	title = {Simple {Techniques} for {Cross}-{Collection} {Relevance} {Feedback}},
	volume = {11437},
	isbn = {978-3-030-15711-1 978-3-030-15712-8},
	url = {http://link.springer.com/10.1007/978-3-030-15712-8_26},
	abstract = {We tackle the problem of transferring relevance judgments across document collections for speciﬁc information needs by reproducing and generalizing the work of Grossman and Cormack from the TREC 2017 Common Core Track. Their approach involves training relevance classiﬁers using human judgments on one or more existing (source) document collections and then applying those classiﬁers to a new (target) document collection. Evaluation results show that their approach, based on logistic regression using word-level tf-idf features, is both simple and eﬀective, with average precision scores close to human-in-the-loop runs. The original approach required inference on every document in the target collection, which we reformulated into a more eﬃcient reranking architecture using widely-available open-source tools. Our eﬀorts to reproduce the TREC results were successful, and additional experiments demonstrate that relevance judgments can be eﬀectively transferred across collections in diﬀerent combinations. We aﬃrm that this approach to cross-collection relevance feedback is simple, robust, and eﬀective.},
	language = {en},
	urldate = {2020-02-27},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Yu, Ruifan and Xie, Yuhao and Lin, Jimmy},
	editor = {Azzopardi, Leif and Stein, Benno and Fuhr, Norbert and Mayr, Philipp and Hauff, Claudia and Hiemstra, Djoerd},
	year = {2019},
	doi = {10.1007/978-3-030-15712-8_26},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {ecir repro track},
	pages = {397--409},
	file = {Yu et al. - 2019 - Simple Techniques for Cross-Collection Relevance F.pdf:files/1147/Yu et al. - 2019 - Simple Techniques for Cross-Collection Relevance F.pdf:application/pdf},
}

@inproceedings{kamphuis_which_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Which {BM25} {Do} {You} {Mean}? {A} {Large}-{Scale} {Reproducibility} {Study} of {Scoring} {Variants}},
	isbn = {978-3-030-45442-5},
	shorttitle = {Which {BM25} {Do} {You} {Mean}?},
	doi = {10.1007/978-3-030-45442-5_4},
	abstract = {When researchers speak of BM25, it is not entirely clear which variant they mean, since many tweaks to Robertson et al.’s original formulation have been proposed. When practitioners speak of BM25, they most likely refer to the implementation in the Lucene open-source search library. Does this ambiguity “matter”? We attempt to answer this question with a large-scale reproducibility study of BM25, considering eight variants. Experiments on three newswire collections show that there are no significant effectiveness differences between them, including Lucene’s often maligned approximation of document length. As an added benefit, our empirical approach takes advantage of databases for rapid IR prototyping, which validates both the feasibility and methodological advantages claimed in previous work.},
	language = {en},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Kamphuis, Chris and de Vries, Arjen P. and Boytsov, Leonid and Lin, Jimmy},
	editor = {Jose, Joemon M. and Yilmaz, Emine and Magalhães, João and Castells, Pablo and Ferro, Nicola and Silva, Mário J. and Martins, Flávio},
	year = {2020},
	keywords = {Relational databases, Scoring functions},
	pages = {28--34},
	file = {Springer Full Text PDF:files/1316/Kamphuis et al. - 2020 - Which BM25 Do You Mean A Large-Scale Reproducibil.pdf:application/pdf},
}

@inproceedings{hasibi_reproducibility_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {On the {Reproducibility} of the {TAGME} {Entity} {Linking} {System}},
	isbn = {978-3-319-30671-1},
	doi = {10.1007/978-3-319-30671-1_32},
	abstract = {Reproducibility is a fundamental requirement of scientific research. In this paper, we examine the repeatability, reproducibility, and generalizability of TAGME, one of the most popular entity linking systems. By comparing results obtained from its public API with (re)implementations from scratch, we obtain the following findings. The results reported in the TAGME paper cannot be repeated due to the unavailability of data sources. Part of the results are reproducible through the provided API, while the rest are not reproducible. We further show that the TAGME approach is generalizable to the task of entity linking in queries. Finally, we provide insights gained during this process and formulate lessons learned to inform future reducibility efforts.},
	language = {en},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Hasibi, Faegheh and Balog, Krisztian and Bratsberg, Svein Erik},
	editor = {Ferro, Nicola and Crestani, Fabio and Moens, Marie-Francine and Mothe, Josiane and Silvestri, Fabrizio and Di Nunzio, Giorgio Maria and Hauff, Claudia and Silvello, Gianmaria},
	year = {2016},
	keywords = {Evaluation Metrics, Input Text, Short Text, Source Code, Test Collection},
	pages = {436--449},
	file = {Springer Full Text PDF:files/1332/Hasibi et al. - 2016 - On the Reproducibility of the TAGME Entity Linking.pdf:application/pdf},
}

@inproceedings{mackie_experiments_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Experiments in {Newswire} {Summarisation}},
	isbn = {978-3-319-30671-1},
	doi = {10.1007/978-3-319-30671-1_31},
	abstract = {In this paper, we investigate extractive multi-document summarisation algorithms over newswire corpora. Examining recent findings, baseline algorithms, and state-of-the-art systems is pertinent given the current research interest in event tracking and summarisation. We first reproduce previous findings from the literature, validating that automatic summarisation evaluation is a useful proxy for manual evaluation, and validating that several state-of-the-art systems with similar automatic evaluation scores create different summaries from one another. Following this verification of previous findings, we then reimplement various baseline and state-of-the-art summarisation algorithms, and make several observations from our experiments. Our findings include: an optimised Lead baseline; indication that several standard baselines may be weak; evidence that the standard baselines can be improved; results showing that the most effective improved baselines are not statistically significantly less effective than the current state-of-the-art systems; and finally, observations that manually optimising the choice of anti-redundancy components, per topic, can lead to improvements in summarisation effectiveness.},
	language = {en},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Mackie, Stuart and McCreadie, Richard and Macdonald, Craig and Ounis, Iadh},
	editor = {Ferro, Nicola and Crestani, Fabio and Moens, Marie-Francine and Mothe, Josiane and Silvestri, Fabrizio and Di Nunzio, Giorgio Maria and Hauff, Claudia and Silvello, Gianmaria},
	year = {2016},
	keywords = {Cosine Similarity, Input Sentence, Leibler Divergence, Standard Baseline, Virtual Document},
	pages = {421--435},
	file = {Springer Full Text PDF:files/1334/Mackie et al. - 2016 - Experiments in Newswire Summarisation.pdf:application/pdf},
}

@inproceedings{lin_toward_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Toward {Reproducible} {Baselines}: {The} {Open}-{Source} {IR} {Reproducibility} {Challenge}},
	isbn = {978-3-319-30671-1},
	shorttitle = {Toward {Reproducible} {Baselines}},
	doi = {10.1007/978-3-319-30671-1_30},
	abstract = {The Open-Source IR Reproducibility Challenge brought together developers of open-source search engines to provide reproducible baselines of their systems in a common environment on Amazon EC2. The product is a repository that contains all code necessary to generate competitive ad hoc retrieval baselines, such that with a single script, anyone with a copy of the collection can reproduce the submitted runs. Our vision is that these results would serve as widely accessible points of comparison in future IR research. This project represents an ongoing effort, but we describe the first phase of the challenge that was organized as part of a workshop at SIGIR 2015. We have succeeded modestly so far, achieving our main goals on the Gov2 collection with seven open-source search engines. In this paper, we describe our methodology, share experimental results, and discuss lessons learned as well as next steps.},
	language = {en},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Lin, Jimmy and Crane, Matt and Trotman, Andrew and Callan, Jamie and Chattopadhyaya, Ishan and Foley, John and Ingersoll, Grant and Macdonald, Craig and Vigna, Sebastiano},
	editor = {Ferro, Nicola and Crestani, Fabio and Moens, Marie-Francine and Mothe, Josiane and Silvestri, Fabrizio and Di Nunzio, Giorgio Maria and Hauff, Claudia and Silvello, Gianmaria},
	year = {2016},
	keywords = {ad hoc retrieval, Open-source search engines},
	pages = {408--420},
	file = {Springer Full Text PDF:files/1336/Lin et al. - 2016 - Toward Reproducible Baselines The Open-Source IR .pdf:application/pdf},
}

@inproceedings{potthast_who_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Who {Wrote} the {Web}? {Revisiting} {Influential} {Author} {Identification} {Research} {Applicable} to {Information} {Retrieval}},
	isbn = {978-3-319-30671-1},
	shorttitle = {Who {Wrote} the {Web}?},
	doi = {10.1007/978-3-319-30671-1_29},
	abstract = {In this paper, we revisit author identification research by conducting a new kind of large-scale reproducibility study: we select 15 of the most influential papers for author identification and recruit a group of students to reimplement them from scratch. Since no open source implementations have been released for the selected papers to date, our public release will have a significant impact on researchers entering the field. This way, we lay the groundwork for integrating author identification with information retrieval to eventually scale the former to the web. Furthermore, we assess the reproducibility of all reimplemented papers in detail, and conduct the first comparative evaluation of all approaches on three well-known corpora.},
	language = {en},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Potthast, Martin and Braun, Sarah and Buz, Tolga and Duffhauss, Fabian and Friedrich, Florian and Gülzow, Jörg Marvin and Köhler, Jakob and Lötzsch, Winfried and Müller, Fabian and Müller, Maike Elisa and Paßmann, Robert and Reinke, Bernhard and Rettenmeier, Lucas and Rometsch, Thomas and Sommer, Timo and Träger, Michael and Wilhelm, Sebastian and Stein, Benno and Stamatatos, Efstathios and Hagen, Matthias},
	editor = {Ferro, Nicola and Crestani, Fabio and Moens, Marie-Francine and Mothe, Josiane and Silvestri, Fabrizio and Di Nunzio, Giorgio Maria and Hauff, Claudia and Silvello, Gianmaria},
	year = {2016},
	keywords = {Domain Expert, Function Word, Information Retrieval, Reproducibility Study, Shared Task},
	pages = {393--407},
	file = {Springer Full Text PDF:files/1338/Potthast et al. - 2016 - Who Wrote the Web Revisiting Influential Author I.pdf:application/pdf},
}

@inproceedings{ferro_rank-biased_2015,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Rank-{Biased} {Precision} {Reloaded}: {Reproducibility} and {Generalization}},
	isbn = {978-3-319-16354-3},
	shorttitle = {Rank-{Biased} {Precision} {Reloaded}},
	doi = {10.1007/978-3-319-16354-3_83},
	abstract = {In this work we reproduce the experiments presented in the paper entitled “Rank-Biased Precision for Measurement of Retrieval Effectiveness”. This paper introduced a new effectiveness measure – Rank- Biased Precision (RBP) – which has become a reference point in the IR experimental evaluation panorama.We will show that the experiments presented in the original RBP paper are repeatable and we discuss points of strength and limitations of the approach taken by the authors. We also present a generalization of the results by adopting four experimental collections and different analysis methodologies.},
	language = {en},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Ferro, Nicola and Silvello, Gianmaria},
	editor = {Hanbury, Allan and Kazai, Gabriella and Rauber, Andreas and Fuhr, Norbert},
	year = {2015},
	keywords = {Average Precision, Experimental Collection, Mean Average Precision, Pool Depth, Reference Paper},
	pages = {768--780},
	file = {Springer Full Text PDF:files/1340/Ferro and Silvello - 2015 - Rank-Biased Precision Reloaded Reproducibility an.pdf:application/pdf},
}

@inproceedings{rao_reproducible_2015,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Reproducible {Experiments} on {Lexical} and {Temporal} {Feedback} for {Tweet} {Search}},
	isbn = {978-3-319-16354-3},
	doi = {10.1007/978-3-319-16354-3_82},
	abstract = {“Evaluation as a service” (EaaS) is a new methodology for community-wide evaluations where an API provides the only point of access to the collection for completing the evaluation task. Two important advantages of this model are that it enables reproducible IR experiments and encourages sharing of pluggable open-source components. In this paper, we illustrate both advantages by providing open-source implementations of lexical and temporal feedback techniques for tweet search built on the TREC Microblog API. For the most part, we are able to reproduce results reported in previous papers and confirm their general findings. However, experiments on new test collections and additional analyses provide a more nuanced look at the results and highlight issues not discussed in previous studies, particularly the large variances in effectiveness associated with training/test splits.},
	language = {en},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Rao, Jinfeng and Lin, Jimmy and Efron, Miles},
	editor = {Hanbury, Allan and Kazai, Gabriella and Rauber, Andreas and Fuhr, Norbert},
	year = {2015},
	keywords = {evaluation as a service, search API, TREC Microblog},
	pages = {755--767},
	file = {Springer Full Text PDF:files/1342/Rao et al. - 2015 - Reproducible Experiments on Lexical and Temporal F.pdf:application/pdf},
}

@inproceedings{hagen_twitter_2015,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Twitter {Sentiment} {Detection} via {Ensemble} {Classification} {Using} {Averaged} {Confidence} {Scores}},
	isbn = {978-3-319-16354-3},
	doi = {10.1007/978-3-319-16354-3_81},
	abstract = {We reproduce three classification approaches with diverse feature sets for the task of classifying the sentiment expressed in a given tweet as either positive, neutral, or negative. The reproduced approaches are also combined in an ensemble, averaging the individual classifiers’ confidence scores for the three classes and deciding sentiment polarity based on these averages. Our experimental evaluation on SemEval data shows our re-implementations to slightly outperform their respective originals. Moreover, in the SemEval Twitter sentiment detection tasks of 2013 and 2014, the ensemble of reproduced approaches would have been ranked in the top-5 among 50 participants. An error analysis shows that the ensemble classifier makes few severe misclassifications, such as identifying a positive sentiment in a negative tweet or vice versa. Instead, it tends to misclassify tweets as neutral that are not, which can be viewed as the safest option.},
	language = {en},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Hagen, Matthias and Potthast, Martin and Büchner, Michel and Stein, Benno},
	editor = {Hanbury, Allan and Kazai, Gabriella and Rauber, Andreas and Fuhr, Norbert},
	year = {2015},
	keywords = {Emotion Lexicon, Ensemble Method, Pointwise Mutual Information, Sentiment Analysis, Twitter Data},
	pages = {741--754},
	file = {Springer Full Text PDF:files/1344/Hagen et al. - 2015 - Twitter Sentiment Detection via Ensemble Classific.pdf:application/pdf},
}

@inproceedings{dur_reproducing_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Reproducing a {Neural} {Question} {Answering} {Architecture} {Applied} to the {SQuAD} {Benchmark} {Dataset}: {Challenges} and {Lessons} {Learned}},
	isbn = {978-3-319-76941-7},
	shorttitle = {Reproducing a {Neural} {Question} {Answering} {Architecture} {Applied} to the {SQuAD} {Benchmark} {Dataset}},
	doi = {10.1007/978-3-319-76941-7_8},
	abstract = {Reproducibility is one of the pillars of scientific research. This study attempts to reproduce the Gated Self-Matching Network, which is the basis of one of the best performing models on the SQuAD dataset. We reimplement the neural network model and highlight ambiguities in the original architectural description. We show that due to uncertainty about only two components of the neural network model and no precise description of the training process, it is not possible to reproduce the experimental results obtained by the original implementation. Finally we summarize what we learned from this reproduction process about writing precise neural network architecture descriptions, providing our implementation as a basis for future exploration.},
	language = {en},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Dür, Alexander and Rauber, Andreas and Filzmoser, Peter},
	editor = {Pasi, Gabriella and Piwowarski, Benjamin and Azzopardi, Leif and Hanbury, Allan},
	year = {2018},
	pages = {102--113},
	file = {Springer Full Text PDF:files/1346/Dür et al. - 2018 - Reproducing a Neural Question Answering Architectu.pdf:application/pdf},
}

@inproceedings{yang_reproducibility_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {On the {Reproducibility} and {Generalisation} of the {Linear} {Transformation} of {Word} {Embeddings}},
	isbn = {978-3-319-76941-7},
	doi = {10.1007/978-3-319-76941-7_20},
	abstract = {Linear transformation is a way to learn a linear relationship between two word embeddings, such that words in the two different embedding spaces can be semantically related. In this paper, we examine the reproducibility and generalisation of the linear transformation of word embeddings. Linear transformation is particularly useful when translating word embedding models in different languages, since it can capture the semantic relationships between two models. We first reproduce two linear transformation approaches, a recent one using orthogonal transformation and the original one using simple matrix transformation. Previous findings on a machine translation task are re-examined, validating that linear transformation is indeed an effective way to transform word embedding models in different languages. In particular, we show that the orthogonal transformation can better relate the different embedding models. Following the verification of previous findings, we then study the generalisation of linear transformation in a multi-language Twitter election classification task. We observe that the orthogonal transformation outperforms the matrix transformation. In particular, it significantly outperforms the random classifier by at least 10\% under the F1 metric across English and Spanish datasets. In addition, we also provide best practices when using linear transformation for multi-language Twitter election classification.},
	language = {en},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Yang, Xiao and Ounis, Iadh and McCreadie, Richard and Macdonald, Craig and Fang, Anjie},
	editor = {Pasi, Gabriella and Piwowarski, Benjamin and Azzopardi, Leif and Hanbury, Allan},
	year = {2018},
	keywords = {Embedding, Linear transformation, Twitter classification},
	pages = {263--275},
	file = {Springer Full Text PDF:files/1348/Yang et al. - 2018 - On the Reproducibility and Generalisation of the L.pdf:application/pdf},
}

@inproceedings{silvello_statistical_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Statistical {Stemmers}: {A} {Reproducibility} {Study}},
	isbn = {978-3-319-76941-7},
	shorttitle = {Statistical {Stemmers}},
	doi = {10.1007/978-3-319-76941-7_29},
	abstract = {Statistical stemmers are important components of Information Retrieval (IR) systems, especially for text search over languages with few linguistic resources. In recent years, research on stemmers produced relevant results, especially in 2011 when three language-independent stemmers were published in relevant venues. In this paper, we describe our efforts for reproducing these three stemmers. We also share the code as open-source and an extended version of Terrier system integrating the developed stemmers.},
	language = {en},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Silvello, Gianmaria and Bucco, Riccardo and Busato, Giulio and Fornari, Giacomo and Langeli, Andrea and Purpura, Alberto and Rocco, Giacomo and Tezza, Alessandro and Agosti, Maristella},
	editor = {Pasi, Gabriella and Piwowarski, Benjamin and Azzopardi, Leif and Hanbury, Allan},
	year = {2018},
	pages = {385--397},
	file = {Springer Full Text PDF:files/1350/Silvello et al. - 2018 - Statistical Stemmers A Reproducibility Study.pdf:application/pdf},
}

@inproceedings{stihec_simplified_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Simplified {Hybrid} {Approach} for {Detection} of {Semantic} {Orientations} in {Economic} {Texts}},
	isbn = {978-3-319-76941-7},
	doi = {10.1007/978-3-319-76941-7_64},
	abstract = {The aim of this work is to reproduce the approach to detecting semantic orientations in economic texts that was presented in the paper Good Debt or Bad Debt: Detecting Semantic Orientations in Economic Texts by Malo et al. The approach employs the Linearized Phrase Structure model for sentence level classification of short economic texts into a positive, negative or neutral category from investor’s perspective and yields state-of-the-art results. The proposed method employs both rule based linguistic models and machine learning. Where possible we follow the same approach as described in the original paper, with some documented modifications. Our solution is simplified in at least two aspects, but its performance is comparable to the original and overall remains better than the reported results of other benchmark algorithms mentioned in the original paper. The differences between the two models and results are described in detail and lead to conclusion that the original approach is to a large extent repeatable and that our simplified version does not overly sacrifice performance for generalizability.},
	language = {en},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Štihec, Jan and Žnidaršič, Martin and Pollak, Senja},
	editor = {Pasi, Gabriella and Piwowarski, Benjamin and Azzopardi, Leif and Hanbury, Allan},
	year = {2018},
	pages = {692--698},
	file = {Springer Full Text PDF:files/1352/Štihec et al. - 2018 - Simplified Hybrid Approach for Detection of Semant.pdf:application/pdf},
}

@inproceedings{mackenzie_compressing_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Compressing {Inverted} {Indexes} with {Recursive} {Graph} {Bisection}: {A} {Reproducibility} {Study}},
	isbn = {978-3-030-15712-8},
	shorttitle = {Compressing {Inverted} {Indexes} with {Recursive} {Graph} {Bisection}},
	doi = {10.1007/978-3-030-15712-8_22},
	abstract = {Document reordering is an important but often overlooked preprocessing stage in index construction. Reordering document identifiers in graphs and inverted indexes has been shown to reduce storage costs and improve processing efficiency in the resulting indexes. However, surprisingly few document reordering algorithms are publicly available despite their importance. A new reordering algorithm derived from recursive graph bisection was recently proposed by Dhulipala et al., and shown to be highly effective and efficient when compared against other state-of-the-art reordering strategies. In this work, we present a reproducibility study of this new algorithm. We describe the implementation challenges encountered, and explore the performance characteristics of our clean-room reimplementation. We show that we are able to successfully reproduce the core results of the original paper, and show that the algorithm generalizes to other collections and indexing frameworks. Furthermore, we make our implementation publicly available to help promote further research in this space.},
	language = {en},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Mackenzie, Joel and Mallia, Antonio and Petri, Matthias and Culpepper, J. Shane and Suel, Torsten},
	editor = {Azzopardi, Leif and Stein, Benno and Fuhr, Norbert and Mayr, Philipp and Hauff, Claudia and Hiemstra, Djoerd},
	year = {2019},
	keywords = {Reproducibility, Compression, Efficiency, Reordering},
	pages = {339--352},
	file = {Springer Full Text PDF:files/1354/Mackenzie et al. - 2019 - Compressing Inverted Indexes with Recursive Graph .pdf:application/pdf},
}

@inproceedings{mallia_experimental_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {An {Experimental} {Study} of {Index} {Compression} and {DAAT} {Query} {Processing} {Methods}},
	isbn = {978-3-030-15712-8},
	doi = {10.1007/978-3-030-15712-8_23},
	abstract = {In the last two decades, the IR community has seen numerous advances in top-k query processing and inverted index compression techniques. While newly proposed methods are typically compared against several baselines, these evaluations are often very limited, and we feel that there is no clear overall picture on the best choices of algorithms and compression methods. In this paper, we attempt to address this issue by evaluating a number of state-of-the-art index compression methods and safe disjunctive DAAT query processing algorithms. Our goal is to understand how much index compression performance impacts overall query processing speed, how the choice of query processing algorithm depends on the compression method used, and how performance is impacted by document reordering techniques and the number of results returned, keeping in mind that current search engines typically use sets of hundreds or thousands of candidates for further reranking.},
	language = {en},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Mallia, Antonio and Siedlaczek, Michał and Suel, Torsten},
	editor = {Azzopardi, Leif and Stein, Benno and Fuhr, Norbert and Mayr, Philipp and Hauff, Claudia and Hiemstra, Djoerd},
	year = {2019},
	keywords = {Compression, Inverted indexes, Query processing},
	pages = {353--368},
	file = {Springer Full Text PDF:files/1356/Mallia et al. - 2019 - An Experimental Study of Index Compression and DAA.pdf:application/pdf},
}

@inproceedings{oosterhuis_optimizing_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Optimizing {Ranking} {Models} in an {Online} {Setting}},
	isbn = {978-3-030-15712-8},
	doi = {10.1007/978-3-030-15712-8_25},
	abstract = {Online Learning to Rank (OLTR) methods optimize ranking models by directly interacting with users, which allows them to be very efficient and responsive. All OLTR methods introduced during the past decade have extended on the original OLTR method: Dueling Bandit Gradient Descent (DBGD). Recently, a fundamentally different approach was introduced with the Pairwise Differentiable Gradient Descent (PDGD) algorithm. To date the only comparisons of the two approaches are limited to simulations with cascading click models and low levels of noise. The main outcome so far is that PDGD converges at higher levels of performance and learns considerably faster than DBGD-based methods. However, the PDGD algorithm assumes cascading user behavior, potentially giving it an unfair advantage. Furthermore, the robustness of both methods to high levels of noise has not been investigated. Therefore, it is unclear whether the reported advantages of PDGD over DBGD generalize to different experimental conditions. In this paper, we investigate whether the previous conclusions about the PDGD and DBGD comparison generalize from ideal to worst-case circumstances. We do so in two ways. First, we compare the theoretical properties of PDGD and DBGD, by taking a critical look at previously proven properties in the context of ranking. Second, we estimate an upper and lower bound on the performance of methods by simulating both ideal user behavior and extremely difficult behavior, i.e., almost-random non-cascading user models. Our findings show that the theoretical bounds of DBGD do not apply to any common ranking model and, furthermore, that the performance of DBGD is substantially worse than PDGD in both ideal and worst-case circumstances. These results reproduce previously published findings about the relative performance of PDGD vs. DBGD and generalize them to extremely noisy and non-cascading circumstances.},
	language = {en},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Oosterhuis, Harrie and de Rijke, Maarten},
	editor = {Azzopardi, Leif and Stein, Benno and Fuhr, Norbert and Mayr, Philipp and Hauff, Claudia and Hiemstra, Djoerd},
	year = {2019},
	keywords = {Gradient descent, Learning to rank, Online learning},
	pages = {382--396},
	file = {Springer Full Text PDF:files/1360/Oosterhuis and de Rijke - 2019 - Optimizing Ranking Models in an Online Setting.pdf:application/pdf},
}

@inproceedings{bhattacharya_comparative_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Comparative} {Study} of {Summarization} {Algorithms} {Applied} to {Legal} {Case} {Judgments}},
	isbn = {978-3-030-15712-8},
	doi = {10.1007/978-3-030-15712-8_27},
	abstract = {Summarization of legal case judgments is an important problem because the huge length and complexity of such documents make them difficult to read as a whole. Many summarization algorithms have been proposed till date, both for general text documents and a few specifically targeted to summarizing legal documents of various countries. However, to our knowledge, there has not been any systematic comparison of the performances of different algorithms in summarizing legal case documents. In this paper, we perform the first such systematic comparison of summarization algorithms applied to legal judgments. We experiment on a large set of Indian Supreme Court judgments, and a large variety of summarization algorithms including both unsupervised and supervised ones. We assess how well domain-independent summarization approaches perform on legal case judgments, and how approaches specifically designed for legal case documents of other countries (e.g., Canada, Australia) generalize to Indian Supreme Court documents. Apart from quantitatively evaluating summaries by comparing with gold standard summaries, we also give important qualitative insights on the performance of different algorithms from the perspective of a law expert.},
	language = {en},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Bhattacharya, Paheli and Hiware, Kaustubh and Rajgaria, Subham and Pochhi, Nilay and Ghosh, Kripabandhu and Ghosh, Saptarshi},
	editor = {Azzopardi, Leif and Stein, Benno and Fuhr, Norbert and Mayr, Philipp and Hauff, Claudia and Hiemstra, Djoerd},
	year = {2019},
	keywords = {Legal case judgment, Summarization, Supervised, Unsupervised},
	pages = {413--428},
	file = {Springer Full Text PDF:files/1364/Bhattacharya et al. - 2019 - A Comparative Study of Summarization Algorithms Ap.pdf:application/pdf},
}

@inproceedings{yates_replicating_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Replicating {Relevance}-{Ranked} {Synonym} {Discovery} in a {New} {Language} and {Domain}},
	isbn = {978-3-030-15712-8},
	doi = {10.1007/978-3-030-15712-8_28},
	abstract = {Domain-specific synonyms occur in many specialized search tasks, such as when searching medical documents, legal documents, and software engineering artifacts. We replicate prior work on ranking domain-specific synonyms in the consumer health domain by applying the approach to a new language and domain: identifying Swedish language synonyms in the building construction domain. We chose this setting because identifying synonyms in this domain is helpful for downstream systems, where different users may query for documents (e.g., engineering requirements) using different terminology. We consider two new features inspired by the change in language and methodological advances since the prior work’s publication. An evaluation using data from the building construction domain supports the finding from the prior work that synonym discovery is best approached as a learning to rank task in which a human editor views ranked synonym candidates in order to construct a domain-specific thesaurus. We additionally find that FastText embeddings alone provide a strong baseline, though they do not perform as well as the strongest learning to rank method. Finally, we analyze the performance of individual features and the differences in the domains.},
	language = {en},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Yates, Andrew and Unterkalmsteiner, Michael},
	editor = {Azzopardi, Leif and Stein, Benno and Fuhr, Norbert and Mayr, Philipp and Hauff, Claudia and Hiemstra, Djoerd},
	year = {2019},
	keywords = {Domain-specific search, Generalization, Replication, Synonym discovery, Thesaurus construction},
	pages = {429--442},
	file = {Springer Full Text PDF:files/1366/Yates and Unterkalmsteiner - 2019 - Replicating Relevance-Ranked Synonym Discovery in .pdf:application/pdf},
}

@inproceedings{manotumruksa_cross-domain_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {On {Cross}-{Domain} {Transfer} in {Venue} {Recommendation}},
	isbn = {978-3-030-15712-8},
	doi = {10.1007/978-3-030-15712-8_29},
	abstract = {Venue recommendation strategies are built upon Collaborative Filtering techniques that rely on Matrix Factorisation (MF), to model users’ preferences. Various cross-domain strategies have been proposed to enhance the effectiveness of MF-based models on a target domain, by transferring knowledge from a source domain. Such cross-domain recommendation strategies often require user overlap, that is common users on the different domains. However, in practice, common users across different domains may not be available. To tackle this problem, recently, several cross-domains strategies without users’ overlaps have been introduced. In this paper, we investigate the performance of state-of-the-art cross-domain recommendation that do not require overlap of users for the venue recommendation task on three large Location-based Social Networks (LBSN) datasets. Moreover, in the context of cross-domain recommendation we extend a state-of-the-art sequential-based deep learning model to boost the recommendation accuracy. Our experimental results demonstrate that state-of-the-art cross-domain recommendation does not clearly contribute to the improvements of venue recommendation systems, and, further we validate this result on the latest sequential deep learning-based venue recommendation approach. Finally, for reproduction purposes we make our implementations publicly available.},
	language = {en},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Manotumruksa, Jarana and Rafailidis, Dimitrios and Macdonald, Craig and Ounis, Iadh},
	editor = {Azzopardi, Leif and Stein, Benno and Fuhr, Norbert and Mayr, Philipp and Hauff, Claudia and Hiemstra, Djoerd},
	year = {2019},
	keywords = {Cross-domain recommendation, Transfer learning, Venue suggestion},
	pages = {443--456},
	file = {Springer Full Text PDF:files/1368/Manotumruksa et al. - 2019 - On Cross-Domain Transfer in Venue Recommendation.pdf:application/pdf},
}

@inproceedings{boratto_effect_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {The {Effect} of {Algorithmic} {Bias} on {Recommender} {Systems} for {Massive} {Open} {Online} {Courses}},
	isbn = {978-3-030-15712-8},
	doi = {10.1007/978-3-030-15712-8_30},
	abstract = {Most recommender systems are evaluated on how they accurately predict user ratings. However, individuals use them for more than an anticipation of their preferences. The literature demonstrated that some recommendation algorithms achieve good prediction accuracy, but suffer from popularity bias. Other algorithms generate an item category bias due to unbalanced rating distributions across categories. These effects have been widely analyzed in the context of books, movies, music, and tourism, but contrasting conclusions have been reached so far. In this paper, we explore how recommender systems work in the context of massive open online courses, going beyond prediction accuracy. To this end, we compared existing algorithms and their recommended lists against biases related to course popularity, catalog coverage, and course category popularity. Our study remarks even more the need of better understanding how recommenders react against bias in diverse contexts.},
	language = {en},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Boratto, Ludovico and Fenu, Gianni and Marras, Mirko},
	editor = {Azzopardi, Leif and Stein, Benno and Fuhr, Norbert and Mayr, Philipp and Hauff, Claudia and Hiemstra, Djoerd},
	year = {2019},
	keywords = {Algorithmic bias, Learning Analytics, Recommendation},
	pages = {457--472},
	file = {Springer Full Text PDF:files/1370/Boratto et al. - 2019 - The Effect of Algorithmic Bias on Recommender Syst.pdf:application/pdf},
}

@inproceedings{berrendorf_knowledge_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Knowledge {Graph} {Entity} {Alignment} with {Graph} {Convolutional} {Networks}: {Lessons} {Learned}},
	isbn = {978-3-030-45442-5},
	shorttitle = {Knowledge {Graph} {Entity} {Alignment} with {Graph} {Convolutional} {Networks}},
	doi = {10.1007/978-3-030-45442-5_1},
	abstract = {In this work, we focus on the problem of entity alignment in Knowledge Graphs (KG) and we report on our experiences when applying a Graph Convolutional Network (GCN) based model for this task. Variants of GCN are used in multiple state-of-the-art approaches and therefore it is important to understand the specifics and limitations of GCN-based models. Despite serious efforts, we were not able to fully reproduce the results from the original paper and after a thorough audit of the code provided by authors, we concluded, that their implementation is different from the architecture described in the paper. In addition, several tricks are required to make the model work and some of them are not very intuitive.We provide an extensive ablation study to quantify the effects these tricks and changes of architecture have on final performance. Furthermore, we examine current evaluation approaches and systematize available benchmark datasets.We believe that people interested in KG matching might profit from our work, as well as novices entering the field. (Code: https://github.com/Valentyn1997/kg-alignment-lessons-learned).},
	language = {en},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Berrendorf, Max and Faerman, Evgeniy and Melnychuk, Valentyn and Tresp, Volker and Seidl, Thomas},
	editor = {Jose, Joemon M. and Yilmaz, Emine and Magalhães, João and Castells, Pablo and Ferro, Nicola and Silva, Mário J. and Martins, Flávio},
	year = {2020},
	pages = {3--11},
	file = {Springer Full Text PDF:files/1372/Berrendorf et al. - 2020 - Knowledge Graph Entity Alignment with Graph Convol.pdf:application/pdf},
}

@inproceedings{frobe_effect_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {The {Effect} of {Content}-{Equivalent} {Near}-{Duplicates} on the {Evaluation} of {Search} {Engines}},
	isbn = {978-3-030-45442-5},
	doi = {10.1007/978-3-030-45442-5_2},
	abstract = {Current best practices for the evaluation of search engines do not take into account duplicate documents. Dependent on their prevalence, not discounting duplicates during evaluation artificially inflates performance scores, and, it penalizes those whose search systems diligently filter them. Although these negative effects have already been demonstrated a long time ago by Bernstein and Zobel [4], we find that this has failed to move the community. In this paper, we reproduce the aforementioned study and extend it to incorporate all TREC Terabyte, Web, and Core tracks. The worst-case penalty of having filtered duplicates in any of these tracks were losses between 8 and 53 ranks.},
	language = {en},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Fröbe, Maik and Bittner, Jan Philipp and Potthast, Martin and Hagen, Matthias},
	editor = {Jose, Joemon M. and Yilmaz, Emine and Magalhães, João and Castells, Pablo and Ferro, Nicola and Silva, Mário J. and Martins, Flávio},
	year = {2020},
	pages = {12--19},
	file = {Springer Full Text PDF:files/1374/Fröbe et al. - 2020 - The Effect of Content-Equivalent Near-Duplicates o.pdf:application/pdf},
}

@inproceedings{grand_maxscore_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {From {MAXSCORE} to {Block}-{Max} {Wand}: {The} {Story} of {How} {Lucene} {Significantly} {Improved} {Query} {Evaluation} {Performance}},
	isbn = {978-3-030-45442-5},
	shorttitle = {From {MAXSCORE} to {Block}-{Max} {Wand}},
	doi = {10.1007/978-3-030-45442-5_3},
	abstract = {The latest major release of Lucene (version 8) in March 2019 incorporates block-max indexes and exploits the block-max variant of Wand for query evaluation, which are innovations that originated from academia. This paper shares the story of how this came to be, which provides an interesting case study at the intersection of reproducibility and academic research achieving impact in the “real world”. We offer additional thoughts on the often idiosyncratic processes by which academic research makes its way into deployed solutions.},
	language = {en},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Grand, Adrien and Muir, Robert and Ferenczi, Jim and Lin, Jimmy},
	editor = {Jose, Joemon M. and Yilmaz, Emine and Magalhães, João and Castells, Pablo and Ferro, Nicola and Silva, Mário J. and Martins, Flávio},
	year = {2020},
	keywords = {Open-source software, Technology adoption},
	pages = {20--27},
	file = {Springer Full Text PDF:files/1376/Grand et al. - 2020 - From MAXSCORE to Block-Max Wand The Story of How .pdf:application/pdf},
}

@inproceedings{kowald_unfairness_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {The {Unfairness} of {Popularity} {Bias} in {Music} {Recommendation}: {A} {Reproducibility} {Study}},
	isbn = {978-3-030-45442-5},
	shorttitle = {The {Unfairness} of {Popularity} {Bias} in {Music} {Recommendation}},
	doi = {10.1007/978-3-030-45442-5_5},
	abstract = {Research has shown that recommender systems are typically biased towards popular items, which leads to less popular items being underrepresented in recommendations. The recent work of Abdollahpouri et al. in the context of movie recommendations has shown that this popularity bias leads to unfair treatment of both long-tail items as well as users with little interest in popular items. In this paper, we reproduce the analyses of Abdollahpouri et al. in the context of music recommendation. Specifically, we investigate three user groups from the Last.fm music platform that are categorized based on how much their listening preferences deviate from the most popular music among all Last.fm users in the dataset: (i) low-mainstream users, (ii) medium-mainstream users, and (iii) high-mainstream users. In line with Abdollahpouri et al., we find that state-of-the-art recommendation algorithms favor popular items also in the music domain. However, their proposed Group Average Popularity metric yields different results for Last.fm than for the movie domain, presumably due to the larger number of available items (i.e., music artists) in the Last.fm dataset we use. Finally, we compare the accuracy results of the recommendation algorithms for the three user groups and find that the low-mainstreaminess group significantly receives the worst recommendations.},
	language = {en},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Kowald, Dominik and Schedl, Markus and Lex, Elisabeth},
	editor = {Jose, Joemon M. and Yilmaz, Emine and Magalhães, João and Castells, Pablo and Ferro, Nicola and Silva, Mário J. and Martins, Flávio},
	year = {2020},
	keywords = {Reproducibility, Algorithmic fairness, Item popularity, Music recommendation, Popularity bias, Recommender systems},
	pages = {35--42},
	file = {Springer Full Text PDF:files/1378/Kowald et al. - 2020 - The Unfairness of Popularity Bias in Music Recomme.pdf:application/pdf},
}

@inproceedings{lin_reproducibility_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Reproducibility is a {Process}, {Not} an {Achievement}: {The} {Replicability} of {IR} {Reproducibility} {Experiments}},
	isbn = {978-3-030-45442-5},
	shorttitle = {Reproducibility is a {Process}, {Not} an {Achievement}},
	doi = {10.1007/978-3-030-45442-5_6},
	abstract = {This paper espouses a view of reproducibility in the computational sciences as a process and not just a point-in-time “achievement”. As a concrete case study, we revisit the Open-Source IR Reproducibility Challenge from 2015 and attempt to replicate those experiments: four years later, are those computational artifacts still functional? Perhaps not surprisingly, we are not able to replicate most of the retrieval runs encapsulated by those artifacts in a modern computational environment. We outline the various idiosyncratic reasons why, distilled into a series of “lessons learned” to help form an emerging set of best practices for the long-term sustainability of reproducibility efforts.},
	language = {en},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Lin, Jimmy and Zhang, Qian},
	editor = {Jose, Joemon M. and Yilmaz, Emine and Magalhães, João and Castells, Pablo and Ferro, Nicola and Silva, Mário J. and Martins, Flávio},
	year = {2020},
	keywords = {Artifact evaluation, Community benchmarks},
	pages = {43--49},
	file = {Springer Full Text PDF:files/1380/Lin and Zhang - 2020 - Reproducibility is a Process, Not an Achievement .pdf:application/pdf},
}

@inproceedings{papariello_replicability_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {On the {Replicability} of {Combining} {Word} {Embeddings} and {Retrieval} {Models}},
	isbn = {978-3-030-45442-5},
	doi = {10.1007/978-3-030-45442-5_7},
	abstract = {We replicate recent experiments attempting to demonstrate an attractive hypothesis about the use of the Fisher kernel framework and mixture models for aggregating word embeddings towards document representations and the use of these representations in document classification, clustering, and retrieval. Specifically, the hypothesis was that the use of a mixture model of von Mises-Fisher (VMF) distributions instead of Gaussian distributions would be beneficial because of the focus on cosine distances of both VMF and the vector space model traditionally used in information retrieval. Previous experiments had validated this hypothesis. Our replication was not able to validate it, despite a large parameter scan space.},
	language = {en},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Papariello, Luca and Bampoulidis, Alexandros and Lupu, Mihai},
	editor = {Jose, Joemon M. and Yilmaz, Emine and Magalhães, João and Castells, Pablo and Ferro, Nicola and Silva, Mário J. and Martins, Flávio},
	year = {2020},
	pages = {50--57},
	file = {Springer Full Text PDF:files/1382/Papariello et al. - 2020 - On the Replicability of Combining Word Embeddings .pdf:application/pdf},
}

@inproceedings{schliski_influence_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Influence of {Random} {Walk} {Parametrization} on {Graph} {Embeddings}},
	isbn = {978-3-030-45442-5},
	doi = {10.1007/978-3-030-45442-5_8},
	abstract = {Network or graph embedding has gained increasing attention in the research community during the last years. In particular, many methods to create graph embeddings using random walk based approaches have been developed. node2vec [10] introduced means to control the random walk behavior, guiding the walks. We aim to reproduce parts of their work and introduce two additional modifications (jump probabilities and attention to hubs), in order to investigate how guiding and modifying the walks influences the learned embeddings. The reproduction includes the case study illustrating homophily and structural equivalence subject to the chosen strategy and a node classification task. We were not able to illustrate structural equivalence and further results show that modifications of the walks only slightly improve node classification, if at all.},
	language = {en},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Schliski, Fabian and Schlötterer, Jörg and Granitzer, Michael},
	editor = {Jose, Joemon M. and Yilmaz, Emine and Magalhães, João and Castells, Pablo and Ferro, Nicola and Silva, Mário J. and Martins, Flávio},
	year = {2020},
	keywords = {Feature learning, Graph embedding, Random walk},
	pages = {58--65},
	file = {Springer Full Text PDF:files/1384/Schliski et al. - 2020 - Influence of Random Walk Parametrization on Graph .pdf:application/pdf},
}

@incollection{hiemstra_cross-domain_2021,
	address = {Cham},
	title = {Cross-{Domain} {Retrieval} in the {Legal} and {Patent} {Domains}: {A} {Reproducibility} {Study}},
	volume = {12657},
	isbn = {978-3-030-72239-5 978-3-030-72240-1},
	shorttitle = {Cross-{Domain} {Retrieval} in the {Legal} and {Patent} {Domains}},
	url = {http://link.springer.com/10.1007/978-3-030-72240-1_1},
	abstract = {Domain speciﬁc search has always been a challenging information retrieval task due to several challenges such as the domain speciﬁc language, the unique task setting, as well as the lack of accessible queries and corresponding relevance judgements. In the last years, pretrained language models – such as BERT – revolutionized web and news search. Naturally, the community aims to adapt these advancements to cross-domain transfer of retrieval models for domain speciﬁc search. In the context of legal document retrieval, Shao et al. propose the BERT-PLI framework by modeling the Paragraph-Level Interactions with the language model BERT. In this paper we reproduce the original experiments, we clarify pre-processing steps and add missing scripts for framework steps, however we are not able to reproduce the evaluation results. Contrary to the original paper, we demonstrate that the domain speciﬁc paragraph-level modelling does not appear to help the performance of the BERT-PLI model compared to paragraph-level modelling with the original BERT. In addition to our legal search reproducibility study, we investigate BERT-PLI for document retrieval in the patent domain. We ﬁnd that the BERT-PLI model does not yet achieve performance improvements for patent document retrieval compared to the BM25 baseline. Furthermore, we evaluate the BERT-PLI model for cross-domain retrieval between the legal and patent domain on individual components, both on a paragraph and document-level. We ﬁnd that the transfer of the BERT-PLI model on the paragraph-level leads to comparable results between both domains as well as ﬁrst promising results for the cross-domain transfer on the document-level. For reproducibility and transparency as well as to beneﬁt the community we make our source code and the trained models publicly available.},
	language = {en},
	urldate = {2021-05-26},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Althammer, Sophia and Hofstätter, Sebastian and Hanbury, Allan},
	editor = {Hiemstra, Djoerd and Moens, Marie-Francine and Mothe, Josiane and Perego, Raffaele and Potthast, Martin and Sebastiani, Fabrizio},
	year = {2021},
	doi = {10.1007/978-3-030-72240-1_1},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {3--17},
	file = {Althammer et al. - 2021 - Cross-Domain Retrieval in the Legal and Patent Dom.pdf:files/1855/Althammer et al. - 2021 - Cross-Domain Retrieval in the Legal and Patent Dom.pdf:application/pdf},
}

@incollection{hiemstra_critical_2021,
	address = {Cham},
	title = {A {Critical} {Assessment} of {State}-of-the-{Art} in {Entity} {Alignment}},
	volume = {12657},
	isbn = {978-3-030-72239-5 978-3-030-72240-1},
	url = {http://link.springer.com/10.1007/978-3-030-72240-1_2},
	abstract = {In this work, we perform an extensive investigation of two state-of-the-art (SotA) methods for the task of Entity Alignment in Knowledge Graphs. Therefore, we ﬁrst carefully examine the benchmarking process and identify several shortcomings, making the results reported in the original works not always comparable. Furthermore, we suspect that it is a common practice in the community to make the hyperparameter optimization directly on a test set, reducing the informative value of reported performance. Thus, we select a representative sample of benchmarking datasets and describe their properties. We also examine diﬀerent initializations for entity representations since they are a decisive factor for model performance. Furthermore, we use a shared train/validation/test split for an appropriate evaluation setting to evaluate all methods on all datasets. In our evaluation, we make several interesting ﬁndings. While we observe that most of the time SotA approaches perform better than baselines, they have diﬃculties when the dataset contains noise, which is the case in most real-life applications. Moreover, in our ablation study, we ﬁnd out that often diﬀerent features of SotA method are crucial for good performance than previously assumed. The code is available at https://github.com/mberr/ea-sota-comparison.},
	language = {en},
	urldate = {2021-05-26},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Berrendorf, Max and Wacker, Ludwig and Faerman, Evgeniy},
	editor = {Hiemstra, Djoerd and Moens, Marie-Francine and Mothe, Josiane and Perego, Raffaele and Potthast, Martin and Sebastiani, Fabrizio},
	year = {2021},
	doi = {10.1007/978-3-030-72240-1_2},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {18--32},
	file = {Berrendorf et al. - 2021 - A Critical Assessment of State-of-the-Art in Entit.pdf:files/1857/Berrendorf et al. - 2021 - A Critical Assessment of State-of-the-Art in Entit.pdf:application/pdf},
}

@incollection{hiemstra_system_2021,
	address = {Cham},
	title = {System {Effect} {Estimation} by {Sharding}: {A} {Comparison} {Between} {ANOVA} {Approaches} to {Detect} {Significant} {Differences}},
	volume = {12657},
	isbn = {978-3-030-72239-5 978-3-030-72240-1},
	shorttitle = {System {Effect} {Estimation} by {Sharding}},
	url = {http://link.springer.com/10.1007/978-3-030-72240-1_3},
	abstract = {The ultimate goal of the evaluation is to understand when two IR systems are (signiﬁcantly) diﬀerent. To this end, many comparison procedures have been developed over time. However, to date, most reproducibility eﬀorts focused just on reproducing systems and algorithms, almost fully neglecting to investigate the reproducibility of the methods we use to compare our systems. In this paper, we focus on methods based on ANalysis Of VAriance (ANOVA), which explicitly model the data in terms of diﬀerent contributing eﬀects, allowing us to obtain a more accurate estimate of signiﬁcant diﬀerences. In this context, recent studies have shown how sharding the corpus can further improve the estimation of the system eﬀect. We replicate and compare methods based on “traditional” ANOVA (tANOVA) to those based on a bootstrapped version of ANOVA (bANOVA) and those performing multiple comparisons relying on a more conservative Family-wise Error Rate (FWER) controlling approach to those relying on a more lenient False Discovery Rate (FDR) controlling approach. We found that bANOVA shows overall a good degree of reproducibility, with some limitations for what concerns the conﬁdence intervals. Besides, compared to the tANOVA approaches, bANOVA presents greater statistical power, at the cost of lower stability. Overall, with this work, we aim at shifting the focus of reproducibility from systems alone to the methods we use to compare and analyze their performance.},
	language = {en},
	urldate = {2021-05-26},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Faggioli, Guglielmo and Ferro, Nicola},
	editor = {Hiemstra, Djoerd and Moens, Marie-Francine and Mothe, Josiane and Perego, Raffaele and Potthast, Martin and Sebastiani, Fabrizio},
	year = {2021},
	doi = {10.1007/978-3-030-72240-1_3},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {33--46},
	file = {Faggioli and Ferro - 2021 - System Effect Estimation by Sharding A Comparison.pdf:files/1859/Faggioli and Ferro - 2021 - System Effect Estimation by Sharding A Comparison.pdf:application/pdf},
}

@incollection{hiemstra_reliability_2021,
	address = {Cham},
	title = {Reliability {Prediction} for {Health}-{Related} {Content}: {A} {Replicability} {Study}},
	volume = {12657},
	isbn = {978-3-030-72239-5 978-3-030-72240-1},
	shorttitle = {Reliability {Prediction} for {Health}-{Related} {Content}},
	url = {https://link.springer.com/10.1007/978-3-030-72240-1_4},
	abstract = {Determining reliability of online data is a challenge that has recently received increasing attention. In particular, unreliable healthrelated content has become pervasive during the COVID-19 pandemic. Previous research [37] has approached this problem with standard classiﬁcation technology using a set of features that have included linguistic and external variables, among others. In this work, we aim to replicate parts of the study conducted by Sondhi and his colleagues using our own code, and make it available for the research community (https://github. com/MarcosFP97/Health-Rel). The performance obtained in this study is as strong as the one reported by the original authors. Moreover, their conclusions are also conﬁrmed by our replicability study. We report on the challenges involved in replication, including that it was impossible to replicate the computation of some features (since some tools or services originally used are now outdated or unavailable). Finally, we also report on a generalisation eﬀort made to evaluate our predictive technology over new datasets [20, 35].},
	language = {en},
	urldate = {2021-05-26},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Fernández-Pichel, Marcos and Losada, David E. and Pichel, Juan C. and Elsweiler, David},
	editor = {Hiemstra, Djoerd and Moens, Marie-Francine and Mothe, Josiane and Perego, Raffaele and Potthast, Martin and Sebastiani, Fabrizio},
	year = {2021},
	doi = {10.1007/978-3-030-72240-1_4},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {47--61},
	file = {Fernández-Pichel et al. - 2021 - Reliability Prediction for Health-Related Content.pdf:files/1861/Fernández-Pichel et al. - 2021 - Reliability Prediction for Health-Related Content.pdf:application/pdf},
}

@incollection{hiemstra_empirical_2021,
	address = {Cham},
	title = {An {Empirical} {Comparison} of {Web} {Page} {Segmentation} {Algorithms}},
	volume = {12657},
	isbn = {978-3-030-72239-5 978-3-030-72240-1},
	url = {http://link.springer.com/10.1007/978-3-030-72240-1_5},
	abstract = {Over the past two decades, several algorithms have been developed to segment a web page into semantically coherent units, a task with several applications in web content analysis. However, these algorithms have hardly been compared empirically and it thus remains unclear which of them—or rather, which of their underlying paradigms—performs best. To contribute to closing this gap, we report on the reproduction and comparative evaluation of ﬁve segmentation algorithms on a large, standardized benchmark dataset for web page segmentation: Three of the algorithms have been speciﬁcally developed for web pages and have been selected to represent paradigmatically different approaches to the task, whereas the other two approaches originate from the segmentation of photos and print documents, respectively. For a fair comparison, we tuned each algorithm’s parameters, if applicable, to the dataset. Altogether, the classic rule-based VIPS algorithm achieved the highest performance, closely followed by the purely visual approach of Cormier et al. For reproducibility, we provide our reimplementations of the algorithms along with detailed instructions.},
	language = {en},
	urldate = {2021-05-26},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Kiesel, Johannes and Meyer, Lars and Kneist, Florian and Stein, Benno and Potthast, Martin},
	editor = {Hiemstra, Djoerd and Moens, Marie-Francine and Mothe, Josiane and Perego, Raffaele and Potthast, Martin and Sebastiani, Fabrizio},
	year = {2021},
	doi = {10.1007/978-3-030-72240-1_5},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {62--74},
	file = {Kiesel et al. - 2021 - An Empirical Comparison of Web Page Segmentation A.pdf:files/1863/Kiesel et al. - 2021 - An Empirical Comparison of Web Page Segmentation A.pdf:application/pdf},
}

@incollection{hiemstra_re-assessing_2021,
	address = {Cham},
	title = {Re-assessing the “{Classify} and {Count}” {Quantification} {Method}},
	volume = {12657},
	isbn = {978-3-030-72239-5 978-3-030-72240-1},
	url = {http://link.springer.com/10.1007/978-3-030-72240-1_6},
	abstract = {Learning to quantify (a.k.a. quantiﬁcation) is a task concerned with training unbiased estimators of class prevalence via supervised learning. This task originated with the observation that “Classify and Count” (CC), the trivial method of obtaining class prevalence estimates, is often a biased estimator, and thus delivers suboptimal quantiﬁcation accuracy. Following this observation, several methods for learning to quantify have been proposed and have been shown to outperform CC. In this work we contend that previous works have failed to use properly optimised versions of CC. We thus reassess the real merits of CC and its variants, and argue that, while still inferior to some cutting-edge methods, they deliver near-state-of-the-art accuracy once (a) hyperparameter optimisation is performed, and (b) this optimisation is performed by using a truly quantiﬁcation-oriented evaluation protocol. Experiments on three publicly available binary sentiment classiﬁcation datasets support these conclusions.},
	language = {en},
	urldate = {2021-05-26},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Moreo, Alejandro and Sebastiani, Fabrizio},
	editor = {Hiemstra, Djoerd and Moens, Marie-Francine and Mothe, Josiane and Perego, Raffaele and Potthast, Martin and Sebastiani, Fabrizio},
	year = {2021},
	doi = {10.1007/978-3-030-72240-1_6},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {75--91},
	file = {Moreo and Sebastiani - 2021 - Re-assessing the “Classify and Count” Quantificati.pdf:files/1865/Moreo and Sebastiani - 2021 - Re-assessing the “Classify and Count” Quantificati.pdf:application/pdf},
}

@incollection{hiemstra_reproducibility_2021,
	address = {Cham},
	title = {Reproducibility, {Replicability} and {Beyond}: {Assessing} {Production} {Readiness} of {Aspect} {Based} {Sentiment} {Analysis} in the {Wild}},
	volume = {12657},
	isbn = {978-3-030-72239-5 978-3-030-72240-1},
	shorttitle = {Reproducibility, {Replicability} and {Beyond}},
	url = {http://link.springer.com/10.1007/978-3-030-72240-1_7},
	abstract = {With the exponential growth of online marketplaces and user-generated content therein, aspect-based sentiment analysis has become more important than ever. In this work, we critically review a representative sample of the models published during the past six years through the lens of a practitioner, with an eye towards deployment in production. First, our rigorous empirical evaluation reveals poor reproducibility: an average 4–5\% drop in test accuracy across the sample. Second, to further bolster our conﬁdence in empirical evaluation, we report experiments on two challenging data slices, and observe a consistent 12–55\% drop in accuracy. Third, we study the possibility of transfer across domains and observe that as little as 10–25\% of the domainspeciﬁc training dataset, when used in conjunction with datasets from other domains within the same locale, largely closes the gap between complete cross-domain and complete in-domain predictive performance. Lastly, we open-source two large-scale annotated review corpora from a large e-commerce portal in India in order to aid the study of replicability and transfer, with the hope that it will fuel further growth of the ﬁeld.},
	language = {en},
	urldate = {2021-05-26},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Mukherjee, Rajdeep and Shetty, Shreyas and Chattopadhyay, Subrata and Maji, Subhadeep and Datta, Samik and Goyal, Pawan},
	editor = {Hiemstra, Djoerd and Moens, Marie-Francine and Mothe, Josiane and Perego, Raffaele and Potthast, Martin and Sebastiani, Fabrizio},
	year = {2021},
	doi = {10.1007/978-3-030-72240-1_7},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {92--106},
	file = {Mukherjee et al. - 2021 - Reproducibility, Replicability and Beyond Assessi.pdf:files/1867/Mukherjee et al. - 2021 - Reproducibility, Replicability and Beyond Assessi.pdf:application/pdf},
}

@incollection{hiemstra_robustness_2021,
	address = {Cham},
	title = {Robustness of {Meta} {Matrix} {Factorization} {Against} {Strict} {Privacy} {Constraints}},
	volume = {12657},
	isbn = {978-3-030-72239-5 978-3-030-72240-1},
	url = {http://link.springer.com/10.1007/978-3-030-72240-1_8},
	abstract = {In this paper, we explore the reproducibility of MetaMF, a meta matrix factorization framework introduced by Lin et al. MetaMF employs meta learning for federated rating prediction to preserve users’ privacy. We reproduce the experiments of Lin et al. on ﬁve datasets, i.e., Douban, Hetrec-MovieLens, MovieLens 1M, Ciao, and Jester. Also, we study the impact of meta learning on the accuracy of MetaMF’s recommendations. Furthermore, in our work, we acknowledge that users may have diﬀerent tolerances for revealing information about themselves. Hence, in a second strand of experiments, we investigate the robustness of MetaMF against strict privacy constraints. Our study illustrates that we can reproduce most of Lin et al.’s results. Plus, we provide strong evidence that meta learning is essential for MetaMF’s robustness against strict privacy constraints.},
	language = {en},
	urldate = {2021-05-26},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Muellner, Peter and Kowald, Dominik and Lex, Elisabeth},
	editor = {Hiemstra, Djoerd and Moens, Marie-Francine and Mothe, Josiane and Perego, Raffaele and Potthast, Martin and Sebastiani, Fabrizio},
	year = {2021},
	doi = {10.1007/978-3-030-72240-1_8},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {107--119},
	file = {Muellner et al. - 2021 - Robustness of Meta Matrix Factorization Against St.pdf:files/1869/Muellner et al. - 2021 - Robustness of Meta Matrix Factorization Against St.pdf:application/pdf},
}

@incollection{hiemstra_textual_2021,
	address = {Cham},
	title = {Textual {Characteristics} of {News} {Title} and {Body} to {Detect} {Fake} {News}: {A} {Reproducibility} {Study}},
	volume = {12657},
	isbn = {978-3-030-72239-5 978-3-030-72240-1},
	shorttitle = {Textual {Characteristics} of {News} {Title} and {Body} to {Detect} {Fake} {News}},
	url = {http://link.springer.com/10.1007/978-3-030-72240-1_9},
	abstract = {Fake news, a deliberately designed news to mislead others, is becoming a big societal threat with its fast dissemination over the Web and social media and its power to shape public opinion. Many researchers have been working to understand the underlying features that help identify these fake news on the Web. Recently, Horne and Adali found, on a small amount of data, that news title stylistic and linguistic features are better than the same type of features extracted from the news body in predicting fake news. In this paper, we present our attempt to reproduce the same results to validate their ﬁndings. We show which of their ﬁndings can be generalized to larger political and gossip news datasets.},
	language = {en},
	urldate = {2021-05-26},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Shrestha, Anu and Spezzano, Francesca},
	editor = {Hiemstra, Djoerd and Moens, Marie-Francine and Mothe, Josiane and Perego, Raffaele and Potthast, Martin and Sebastiani, Fabrizio},
	year = {2021},
	doi = {10.1007/978-3-030-72240-1_9},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {120--133},
	file = {Shrestha and Spezzano - 2021 - Textual Characteristics of News Title and Body to .pdf:files/1871/Shrestha and Spezzano - 2021 - Textual Characteristics of News Title and Body to .pdf:application/pdf},
}

@incollection{hiemstra_federated_2021,
	address = {Cham},
	title = {Federated {Online} {Learning} to {Rank} with {Evolution} {Strategies}: {A} {Reproducibility} {Study}},
	volume = {12657},
	isbn = {978-3-030-72239-5 978-3-030-72240-1},
	shorttitle = {Federated {Online} {Learning} to {Rank} with {Evolution} {Strategies}},
	url = {http://link.springer.com/10.1007/978-3-030-72240-1_10},
	abstract = {Online Learning to Rank (OLTR) optimizes ranking models using implicit users’ feedback, such as clicks, directly manipulating search engine results in production. This process requires OLTR methods to collect user queries and clicks; current methods are not suited to situations in which users want to maintain their privacy, i.e. not sharing data, queries and clicks.},
	language = {en},
	urldate = {2021-05-26},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Wang, Shuyi and Zhuang, Shengyao and Zuccon, Guido},
	editor = {Hiemstra, Djoerd and Moens, Marie-Francine and Mothe, Josiane and Perego, Raffaele and Potthast, Martin and Sebastiani, Fabrizio},
	year = {2021},
	doi = {10.1007/978-3-030-72240-1_10},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {134--149},
	file = {Wang et al. - 2021 - Federated Online Learning to Rank with Evolution S.pdf:files/1873/Wang et al. - 2021 - Federated Online Learning to Rank with Evolution S.pdf:application/pdf},
}

@incollection{hiemstra_comparing_2021,
	address = {Cham},
	title = {Comparing {Score} {Aggregation} {Approaches} for {Document} {Retrieval} with {Pretrained} {Transformers}},
	volume = {12657},
	isbn = {978-3-030-72239-5 978-3-030-72240-1},
	url = {http://link.springer.com/10.1007/978-3-030-72240-1_11},
	abstract = {While BERT has been shown to be eﬀective for passage retrieval, its maximum input length limitation poses a challenge when applying the model to document retrieval. In this work, we reproduce three passage score aggregation approaches proposed by Dai and Callan [5] for overcoming this limitation. After reproducing their results, we generalize their ﬁndings through experiments with a new dataset and experiment with other pretrained transformers that share similarities with BERT. We ﬁnd that these BERT variants are not more eﬀective for document retrieval in isolation, but can lead to increased eﬀectiveness when combined with “pre–ﬁne-tuning” on the MS MARCO passage dataset. Finally, we investigate whether there is a diﬀerence between ﬁnetuning models on “deep” judgments (i.e., fewer queries with many judgments each) vs. ﬁne-tuning on “shallow” judgments (i.e., many queries with fewer judgments each). Based on available data from two diﬀerent datasets, we ﬁnd that the two approaches perform similarly.},
	language = {en},
	urldate = {2021-05-26},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Zhang, Xinyu and Yates, Andrew and Lin, Jimmy},
	editor = {Hiemstra, Djoerd and Moens, Marie-Francine and Mothe, Josiane and Perego, Raffaele and Potthast, Martin and Sebastiani, Fabrizio},
	year = {2021},
	doi = {10.1007/978-3-030-72240-1_11},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {150--163},
	file = {Zhang et al. - 2021 - Comparing Score Aggregation Approaches for Documen.pdf:files/1875/Zhang et al. - 2021 - Comparing Score Aggregation Approaches for Documen.pdf:application/pdf},
}
